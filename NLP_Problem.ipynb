{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqQjThqAo8PBCG0hMs35is",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danykoud/Test-/blob/main/NLP_Problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRMEGDU2aZIz"
      },
      "source": [
        "import pandas as pd\n",
        "import httpimport\n",
        "\n",
        "with httpimport.remote_repo(['lm_helper'], 'https://raw.githubusercontent.com/jasoriya/CS6120-PS2-support/master/utils/'):\n",
        "  from lm_helper import get_train_data, get_test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX0SM5GMaze5"
      },
      "source": [
        "#  get the train and test data\n",
        "train = get_train_data()\n",
        "test, test_files = get_test_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0qMlVQpbLy3"
      },
      "source": [
        "TASK - 1 = 10 points :\n",
        "##**DESCRIPTION** :\n",
        "Collect statistics on the unigram, bigram, and trigram character counts.\n",
        "Statistics such as:\n",
        "a)Length of unigram vocabulary\n",
        "b)Count of total unigrams\n",
        "c)Length of bigram vocab\n",
        "d)Count of total bigrams\n",
        "e)Length of trigram vocabulary\n",
        "f)Count of total trigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bwpFeu2azh4"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# formatting the doc to be used for training\n",
        "\n",
        "tList = []\n",
        "for each in train:\n",
        "    for l in each:\n",
        "        tList.append(\" \".join(l))\n",
        "docList= tList[:8000]\n",
        "def getUnigramVals(docList):\n",
        "    '''\n",
        "\n",
        "    Generate a dictionary with unigrams as keys and their counts for given input as values\n",
        "    e.g: {'a':53,'b':65}\n",
        "    \n",
        "    '''\n",
        "    unigram_vectorizer = CountVectorizer() #use this variable for defining CountVectorizer()\n",
        "    unigram_vector = unigram_vectorizer.fit_transform(docList) #use this variable for fit transform\n",
        "    unigram_counts =unigram_vector.toarray().sum(axis=0)\n",
        "     #use this counting unigrams\n",
        "    return dict(zip(unigram_vectorizer.get_feature_names(),unigram_counts))\n",
        "\n",
        "\n",
        "def getBigramVals(docList):\n",
        "    '''\n",
        "\n",
        "    Generate a dictionary with unigrams as keys and their counts for given input as values\n",
        "    e.g: {'ab':53,'bg':65}\n",
        "    \n",
        "    '''\n",
        "    bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "    bigram_vector =bigram_vectorizer.fit_transform(docList)\n",
        "    bigram_counts =bigram_vector.toarray().sum(axis=0)\n",
        "    return dict(zip(bigram_vectorizer.get_feature_names(), bigram_counts))\n",
        "\n",
        "\n",
        "def getTrigramVals(docList):\n",
        "    '''\n",
        "\n",
        "    Generate a dictionary with unigrams as keys and their counts for given input as values\n",
        "    e.g: {'abd':53,'bgv':65}\n",
        "    \n",
        "    '''\n",
        "    trigram_vectorizer =  CountVectorizer(ngram_range=(3, 3))\n",
        "    trigram_vector = trigram_vectorizer.fit_transform(docList)\n",
        "    trigram_counts = trigram_vector.toarray().sum(axis=0)\n",
        "    return dict(zip((trigram_vectorizer.get_feature_names()), (trigram_counts)))\n",
        "\n",
        "# Get the respective uni,i and trigram values for the entire training data.\n",
        "uniGram = getUnigramVals(docList)\n",
        "biGram = getBigramVals(docList)\n",
        "triGram = getTrigramVals(docList)\n",
        "unigram_sum = sum(uniGram.values())\n",
        "\n",
        "# Print the values of uni, tri and bigrams.\n",
        "print(\"Length of unigram vocab:\" + str(len(uniGram)))\n",
        "print(\"Count of total unigrams:\" + str(sum(uniGram.values())))\n",
        "print(\"Length of bigram vocab:\" + str(len(biGram)))\n",
        "print(\"Count of total bigrams:\" + str(sum(biGram.values())))\n",
        "print(\"Length of trigram vocab:\" + str(len(triGram)))\n",
        "print(\"Count of total trigrams:\" + str(sum(triGram.values())))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N33iLwCzisH3"
      },
      "source": [
        "TASK - 2 = 15 points:\n",
        "##**DESCRIPTION** :\n",
        "Calculate the perplexity for each document in the test set using the linear interpolation smoothing method. For determining Œªs for linear interpolation of the trigram, bigram, and unigram models, you can divide the training data into a new training set (80%) and a held-out set (20%). Then choose ~10 random pairs of  (ùúÜ3,ùúÜ2)  such that  1>ùúÜ3>ùúÜ2>0  and  ‚àë3ùëñ=1ùúÜùëñ=1  to test on held-out data.\n",
        "\n",
        "Some documents in the test set are in Brazilian Portuguese. Identify them as follows:\n",
        "\n",
        "Sort by perplexity and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese.\n",
        "\n",
        "Print the file names (from test_files) and perplexities of the documents above the threshold\n",
        "\n",
        "    file name, score\n",
        "    file name, score\n",
        "    . . .\n",
        "    file name, score\n",
        "Copy this list of filenames and manually annotate them as being correctly or incorrectly labeled as Portuguese."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tobJolZFazmG"
      },
      "source": [
        "# Your code here\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "# split the training data into 80-20 ratio for finding lambdas  \n",
        "docList_train, docList_test = train_test_split( docList, test_size=0.20, random_state=42) # use train_test_split on docList \n",
        "                               \n",
        " \n",
        "# Respective uni, bi and trigrams for the 80 splitted training data\n",
        "unigram_trained = getUnigramVals(docList_train)\n",
        "bigram_trained = getBigramVals(docList_train)\n",
        "trigram_trained = getTrigramVals(docList_train)\n",
        "unigram_trained_sum = sum(unigram_trained.values())\n",
        "bigram_trained_sum = sum(bigram_trained.values())\n",
        "trigram_trained_sum = sum(trigram_trained.values())\n",
        "\n",
        "# Uni, bi and trigram probability computation methods. \n",
        "def getUnigramProbabilitty(l1, uniVals, unigramSum):\n",
        "    \n",
        "    if l1 not in uniVals:\n",
        "      uniVals[l1]=1\n",
        "      # return 0\n",
        "    return uniVals[l1]/unigramSum\n",
        "\n",
        "def getBigramProbability(l1, l2, uniVals, biVals):\n",
        "    pair = l1,l2\n",
        "    pair=\" \".join(pair)\n",
        "    if pair not in biVals:\n",
        "      biVals[pair]=1\n",
        "    if l1 not in uniVals:\n",
        "      uniVals[l1]=1\n",
        "    # if pair not in biVals or l1 not in uniVals:\n",
        "    #   return 0\n",
        "    return biVals[pair]/uniVals[l1]\n",
        "    \n",
        "def getTrigramProbability(l1, l2, l3, triVals, biVals):\n",
        "    tr= l1,l2,l3\n",
        "    pair= l1,l2\n",
        "    pair=\" \".join(pair)\n",
        "    tr=\" \".join(tr) \n",
        "    if tr not in triVals:\n",
        "      triVals[tr]=1\n",
        "    if pair not in biVals:\n",
        "      biVals[pair]= 1\n",
        "    # if tr not in triVals or pair not in biVals:\n",
        "    #   return 0\n",
        "    return triVals[tr]/biVals[pair]\n",
        "# compute the perplexity with linear interpolation   \n",
        "def computePerplexityLinearInterpolation(doc, lambda1, lambda2, lambda3, uniVals, biVals, triVals, uniSum):\n",
        "    entropy = 0\n",
        "    for i in range(0, len(doc)-2):\n",
        "        l1 = doc[i]\n",
        "        l2 = doc[i+1]\n",
        "        l3 = doc[i+2]\n",
        "        p3=getTrigramProbability(l1, l2, l3, triVals, biVals)\n",
        "        p2=getBigramProbability(l1, l2, uniVals, biVals)\n",
        "        p1=getUnigramProbabilitty(l1, uniVals, uniSum)\n",
        "        probability = lambda1*p3 + lambda2*p2 + lambda3*p1\n",
        "        entropy += (- probability * np.log2(probability))\n",
        "    perplexity = 2** entropy\n",
        "    return perplexity\n",
        "\n",
        "# Make the held-out documents i.e books into a single stream or string of data as per the process followed in eisenstein for perplexity computation. Page: 139\n",
        "held_out_doc = \" \".join(docList_test)\n",
        "# held_out_doc = docList_test.strip().split()                            #nltk.word_tokenize(docList_test)\n",
        "\n",
        "lambdasToTry =list() #  eg : [[0.1, 0.1, 0.8],[0.2,0.35,0.45],.........]]    \n",
        "k = np.round(np.random.random(100),2)\n",
        "for i in range(len(held_out_doc[:2000])):\n",
        "    d1= random.choice(k)\n",
        "    d2= random.choice(k)\n",
        "    d3= random.choice(k)\n",
        "    if d1+ d2+d3 ==1 and d1!= d2!=d3  :\n",
        "      lambdasToTry.append([d1, d2,d3])\n",
        "# list for held-out perplexities\n",
        "perps = []\n",
        "for a in lambdasToTry:\n",
        "    lambda1, lambda2, lambda3 = a[0], a[1], a[2]\n",
        "    perps.append(computePerplexityLinearInterpolation(held_out_doc, lambda1, lambda2, lambda3, unigram_trained, bigram_trained, trigram_trained,unigram_trained_sum))\n",
        "\n",
        "print(perps)\n",
        "\n",
        "# final lambdas based on minimum perplexities on held-out data\n",
        "finalLambdas = min(perps)\n",
        "print(finalLambdas)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZuZSGrOazph"
      },
      "source": [
        "testDocList = []\n",
        "#create the test data list in structured format\n",
        "docList_test =\" \".join(docList_test)\n",
        "docList_test = docList_test.strip().split()\n",
        "for test in docList_test:\n",
        "  testDocList.append(test)\n",
        "\n",
        "#list for storing perplexities of test docs\n",
        "perplexities_test_docs_interpolation = []\n",
        "for word in testDocList:\n",
        "  perplexities_test_docs_interpolation.append(computePerplexityLinearInterpolation(word, lambda1, lambda2, lambda3, unigram_trained, bigram_trained, trigram_trained,unigram_trained_sum))\n",
        "print(perplexities_test_docs_interpolation)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7Z6uzeqazsv"
      },
      "source": [
        "#sort test docs based on perplexity on perplexities_test_docs_interpolation\n",
        "testDocperps = {}\n",
        "for word in docList_test:\n",
        "  for perp in perplexities_test_docs_interpolation:\n",
        "    testDocperps[word]= float(perp)\n",
        "sorted(testDocperps)\n",
        "#print all perplexities of docs in sorted order to find threshold\n",
        "print(testDocperps)\n",
        "\n",
        "\n",
        "#set the perplexity threshold based on observation after sorted values are printed.\n",
        "perplexity_threshold_for_interpolation = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NjmKuWHazvu"
      },
      "source": [
        "#Classify non english docs based on perplexity threshold and printing their filenames along with perplexity for manual validation on perplexities_test_docs_interpolation\n",
        "\n",
        "..........."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7s08PIPazyu"
      },
      "source": [
        "#print few lines from the doc, perplexity value, filename and classification by the model by using perplexities_test_docs_interpolation\n",
        ".........."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d35naOY1IlwR"
      },
      "source": [
        "**TASK** - 3 = 10 points :\n",
        "##**DESCRIPTION** :\n",
        "Build a trigram language model with add-Œª smoothing (use Œª = 0.1).\n",
        "\n",
        "Sort the test documents by perplexity and perform a check for Brazilian Portuguese documents as above:\n",
        "\n",
        "Observe the perplexity scores and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese.\n",
        "\n",
        "Print the file names and perplexities of the documents above the threshold\n",
        "\n",
        "  file name, score\n",
        "  file name, score\n",
        "  . . .\n",
        "  file name, score\n",
        "Copy this list of filenames and manually annotate them for correctness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRXHWtSKaz2W"
      },
      "source": [
        "# Method for calculating trigram probability \n",
        "def getTrigramProbabilityWithLambdaSmoothing(l1, l2, l3, triVals, biVals, lambdaVal): \n",
        "    tr= l1,l2,l3\n",
        "    pair= l1,l2\n",
        "    pair=\" \".join(pair)\n",
        "    tr=\" \".join(tr) \n",
        "    if tr not in triVals:\n",
        "      triVals[tr]=1\n",
        "    if pair not in biVals:\n",
        "      biVals[pair]= 1\n",
        "    # if tr not in triVals or pair not in biVals:\n",
        "    #   return 0\n",
        "    return (triVals[tr]+ 1)/(biVals[pair]+ lambdaVal )\n",
        " \n",
        "            \n",
        "\n",
        "# Method for perplexity calculation with lambda smoothing\n",
        "def computePerplexityWithLambdaSmoothing(doc, biVals, triVals):\n",
        "    entropy = 0\n",
        "    for i in range(len(doc)-2):\n",
        "        l1 = doc[i]\n",
        "        l2 = doc[i+1]\n",
        "        l3 = doc[i+2]\n",
        "        probability = getTrigramProbabilityWithLambdaSmoothing(l1, l2, l3, triVals, biVals, 0.1)\n",
        "        entropy += (- np.log2(probability))\n",
        "    perplexity = 2**entropy \n",
        "    return perplexity\n",
        "\n",
        "#Testing lambda smoothened trigram perplexity on held-out data\n",
        "#print(computePerplexity(held_out_doc, bigramValsTrain, bigramVocabTrain, trigramValsTrain, trigramVocabTrain))\n",
        "\n",
        "# list for perplexities and details in test data\n",
        "perplexities_test_docs_lambda_smoothing = []\n",
        "for each in testDocList:\n",
        "     perplexities_test_docs_lambda_smoothing.append(computePerplexityWithLambdaSmoothing(each, bigramValsTrain, bigramVocabTrain, trigramValsTrain, trigramVocabTrain))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vNSgsZhaz6F"
      },
      "source": [
        "#sort perplexities_test_docs_lambda_smoothing based on perplexity \n",
        ".......\n",
        "\n",
        "#print all the perplexities in sorted order to figure out threshold\n",
        ".......\n",
        "\n",
        "#perplexity threshold set based on observation after sorted values are printed.\n",
        "perplexity_threshold_for_lambda_smoothing = #select value here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1N88bjoaz92"
      },
      "source": [
        "\n",
        "#Classify non english docs based on perplexity threshold and print their filenames along with perplexity for manual validation on perplexities_test_docs_lambda_smoothing\n",
        "......."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpiyhWfZa0Ar"
      },
      "source": [
        "#print few lines from the doc, perplexity value, filename and classification by the model perplexities_test_docs_lambda_smoothing.\n",
        "..........."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA4nz7OwJJus"
      },
      "source": [
        "**TASK** - 4 = 5 points\n",
        "Based on your observation from above questions, compare linear interpolation and add-Œª smoothing by listing out their pros and cons.\n",
        "\n",
        "**END OF PART-A**\n",
        "\n",
        "**PART-B**\n",
        "\n",
        "**OBJECTIVE** :\n",
        "\n",
        "In this part, you will be implementing logistic regression for sentiment analysis on tweets.\n",
        "\n",
        "Problem Statement :\n",
        "Given a tweet, you will decide if it has a positive sentiment or a negative.\n",
        "\n",
        "Specifically you will:\n",
        "Learn how to extract features for logistic regression given some text\n",
        "Implement logistic regression from scratch\n",
        "Apply logistic regression on a natural language processing task\n",
        "Test using your logistic regression\n",
        "Predict your own tweet.\n",
        "Note : \n",
        "Initial steps will be same as in Assignment-1. You will be working on a data set of tweets.\n",
        "¬∂"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrFJiI4na0Df"
      },
      "source": [
        "import re\n",
        "import string\n",
        "from os import getcwd\n",
        "\n",
        "import pdb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "import matplotlib.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0g_9cC1Jj-d"
      },
      "source": [
        "**Task 0.1 - Preprocessing the Tweets**: ( 2.5 points)\n",
        "\n",
        "In this task you have to start with cleaning the tweet for which you will write a function named clean_tweet which will remove hyperlinks, noise, punctuations, hashtags, old style retweet text \"RT\", share market tickers like $FB, use stemming to only keep track of one variation of each word and in the end will tokenize them. Look for the hint in imports to tokenize the tweet. Then you will write a helper function for determining the word frequency after which you will split the data into training and validation set. While doing this do not make unnceseaary assumptions regarding the lengths of tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY7gC_NUa0G7"
      },
      "source": [
        "#Preprocessing tweets\n",
        "def process_tweet(tweet):\n",
        "    #Remove old style retweet text \"RT\"\n",
        "    tweet2 = re.sub(r'^RT[\\s]','', tweet)\n",
        "    \n",
        "    #Remove hyperlinks\n",
        "    tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*','', tweet2)\n",
        "    \n",
        "    #Remove hastags\n",
        "    #Only removing the hash # sign from the word\n",
        "    tweet2 = re.sub(r'#','',tweet2)\n",
        "        \n",
        "    # instantiate tokenizer class\n",
        "    tokenizer = TweetTokenizer(preserve_case=False,    strip_handles=True, reduce_len=True)\n",
        "    \n",
        "    # tokenize tweets\n",
        "    tweet_tokens = tokenizer.tokenize(tweet2)    \n",
        "        \n",
        "    #Import the english stop words list from NLTK\n",
        "    stopwords_english = stopwords.words('english') \n",
        "    \n",
        "    #Creating a list of words without stopwords\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if word not in stopwords_english and word not in string.punctuation:\n",
        "            tweets_clean.append(word)\n",
        "        \n",
        "    #Instantiate stemming class\n",
        "    stemmer = PorterStemmer()\n",
        "    \n",
        "    #Creating a list of stems of words in tweet\n",
        "    tweets_stem = []\n",
        "    for word in tweets_clean:\n",
        "        stem_word = stemmer.stem(word)\n",
        "        tweets_stem.append(stem_word)\n",
        "        \n",
        "    return tweets_stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zotwUV2a0Mf"
      },
      "source": [
        "# TASK 1 CELL\n",
        "\n",
        "# Helper function to find word frequency\n",
        "def find(frequency, word, label):\n",
        "    '''\n",
        "    Params:\n",
        "        frequency: a dictionary with the frequency of each pair (or tuple)\n",
        "        word: the word to look up\n",
        "        label: the label corresponding to the word\n",
        "    Return:\n",
        "        n: the number of times the word with its corresponding label appears.\n",
        "    '''\n",
        "    n = 0  \n",
        "    \n",
        "    pair = (word, label)\n",
        "    if pair in frequency:\n",
        "        n = frequency[pair]\n",
        "    return n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiXYJBxEKBbO"
      },
      "source": [
        "If you are running this notebook in your local computer, don't forget to download the twitter samples and stopwords from nltk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aisc7ENza0Q3"
      },
      "source": [
        "# Run this cell\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-RnBJt6a0Va"
      },
      "source": [
        "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uYazuJYa0aC"
      },
      "source": [
        "# get the sets of positive and negative tweets\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# split the data into train and validation set\n",
        "test_positive = positive_tweets[4000:]\n",
        "train_positive = positive_tweets[:4000]\n",
        "test_negative = all_negative_tweets[4000:]\n",
        "train_negative = all_negative_tweets[:4000]\n",
        "train_x = train_positive + train_negative\n",
        "test_x = test_positive + test_negative\n",
        "train_y = np.append(np.ones((len(train_positive), 1)), np.zeros((len(train_negative), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_positive), 1)), np.zeros((len(test_negative), 1)), axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W8c0d2ybUGk"
      },
      "source": [
        "tweets= pd.DataFrame.from_dict(dict(tweet=train_x, label=train_y.ravel()))\n",
        "tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUq81RRYIjch"
      },
      "source": [
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUxTr62fKYUR"
      },
      "source": [
        "### **Task 0.2 - Tweet Counter: (2.5 points)** \n",
        "\n",
        "\n",
        "To help train your naive bayes model, you will need to build a dictionary where the keys are a (word, label) tuple and the values are the corresponding frequency.  Note that the labels we'll use here are 1 for positive and 0 for negative.\n",
        "\n",
        "You will also implement a `find()` helper function that takes in the `freqs` dictionary, a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets.\n",
        "\n",
        "For example: given a list of tweets `[\"i am rather excited\", \"you are rather happy\"]` and the label 1, the function will return a dictionary that contains the following key-value pairs:\n",
        "\n",
        "{\n",
        "    (\"rather\", 1): 2\n",
        "    (\"happi\", 1) : 1\n",
        "    (\"excit\", 1) : 1\n",
        "}\n",
        "\n",
        "- Notice how for each word in the given string, the same label 1 is assigned to each word.\n",
        "- Notice how the words \"i\" and \"am\" are not saved, since it was removed by clean_tweet because it is a stopword.\n",
        "- Notice how the word \"rather\" appears twice in the list of tweets, and so its count value is 2.\n",
        "\n",
        "#### **Instructions**\n",
        "Create a function `tweet_counter()` that takes a list of tweets as input, cleans all of them, and returns a dictionary.\n",
        "- The key in the dictionary is a tuple containing the stemmed word and its class label, e.g. (\"happi\",1).\n",
        "- The value the number of times this word appears in the given collection of tweets (an integer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hSdvCyfa0eG"
      },
      "source": [
        "# TASK 2 CELL\n",
        "\n",
        "def tweet_counter(output, tweets, tweet_senti):\n",
        "    '''\n",
        "    Params:\n",
        "        output: a dictionary that will be used to map each pair to its frequency\n",
        "        tweets: a list of tweets\n",
        "        tweet_senti: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
        "    Return:\n",
        "        output: a dictionary mapping each pair to its frequency\n",
        "    '''\n",
        "    # method#1\n",
        "    # for label, tweet in zip(tweet_senti, tweets):\n",
        "    #   for word in process_tweet(tweet):\n",
        "    #     pair = (word, label)\n",
        "    #     output[pair] = output.get(pair, 0) + 1\n",
        "    # method#2\n",
        "    for label, tweet in zip(tweet_senti, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair=(word,label)\n",
        "            if pair in output:\n",
        "                output[pair] += 1\n",
        "            else:\n",
        "              output[pair] = 1\n",
        "\n",
        "    return output\n",
        "\n",
        "frequency_dict= freqs= tweet_counter({}, train_x, train_y.ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7CSttTEK3G7"
      },
      "source": [
        "##**Logistic regression**\n",
        "¬∂\n",
        "Task-1 = 15 Points\n",
        "Task 1.1: Sigmoid\n",
        "The sigmoid function is defined as:\n",
        "\n",
        "‚Ñé(ùëß)=1/1+exp‚àíùëß(1)\n",
        "\n",
        "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V5SqTwAa0hh"
      },
      "source": [
        "import math\n",
        "def sigmoid(z): \n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "    \n",
        "   \n",
        "    # calculate the sigmoid of z\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    \n",
        "    return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMFdzpB6M2F1"
      },
      "source": [
        "# Test your sigmoid function by taking z=0 and z = 4.92. check and print if it works correctly as success.\n",
        "if (sigmoid(0) == 0.5):\n",
        "    print('SUCCESS!')\n",
        "else:\n",
        "    print('Oops!')\n",
        "\n",
        "if (sigmoid(4.92) == 0.9927537604041685):\n",
        "    print('CORRECT!')\n",
        "else:\n",
        "    print('Oops again!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbE7FGU-LLkq"
      },
      "source": [
        "**Logistic regression**: \n",
        "\n",
        "regression and a sigmoid\n",
        "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
        "\n",
        "Regression:\n",
        "\n",
        "ùëß=ùúÉ0ùë•0+ùúÉ1ùë•1+ùúÉ2ùë•2+...ùúÉùëÅùë•ùëÅ\n",
        " \n",
        "Note that the  ùúÉ  values are \"weights\". If you took the Deep Learning Specialization, we referred to the weights with the w vector. In this course, we're using a different variable  ùúÉ  to refer to the weights.\n",
        "\n",
        "Logistic regression\n",
        "\n",
        "‚Ñé(ùëß)=11+exp‚àíùëß\n",
        " \n",
        "ùëß=ùúÉ0ùë•0+ùúÉ1ùë•1+ùúÉ2ùë•2+...ùúÉùëÅùë•ùëÅ\n",
        " \n",
        "We will refer to 'z' as the 'logits'.\n",
        "\n",
        "Task 1.2 Cost function and Gradient\n",
        "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
        "\n",
        "ùêΩ(ùúÉ)=‚àí1ùëö‚àëùëñ=1ùëöùë¶(ùëñ)log(‚Ñé(ùëß(ùúÉ)(ùëñ)))+(1‚àíùë¶(ùëñ))log(1‚àí‚Ñé(ùëß(ùúÉ)(ùëñ)))(5)\n",
        "\n",
        "ùëö is the number of training examples\n",
        "ùë¶(ùëñ) is the actual label of the i-th training example.\n",
        "‚Ñé(ùëß(ùúÉ)(ùëñ)) is the model's prediction for the i-th training example.\n",
        "The loss function for a single training example is\n",
        "\n",
        "ùêøùëúùë†ùë†=‚àí1√ó(ùë¶(ùëñ)log(‚Ñé(ùëß(ùúÉ)(ùëñ)))+(1‚àíùë¶(ùëñ))log(1‚àí‚Ñé(ùëß(ùúÉ)(ùëñ))))\n",
        "\n",
        "All the ‚Ñé values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
        "Note that when the model predicts 1 (‚Ñé(ùëß(ùúÉ))=1) and the label ùë¶ is also 1, the loss for that training example is 0.\n",
        "Similarly, when the model predicts 0 (‚Ñé(ùëß(ùúÉ))=0) and the actual label is also 0, the loss for that training example is 0.\n",
        "However, when the model prediction is close to 1 (‚Ñé(ùëß(ùúÉ))=0.9999) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. ‚àí1√ó(1‚àí0)√óùëôùëúùëî(1‚àí0.9999)‚âà9.2 The closer the model prediction gets to 1, the larger the loss.\n",
        "Update the weights\n",
        "To update your weight vector ùúÉ, you will apply gradient descent to iteratively improve your model's predictions.\n",
        "The gradient of the cost function ùêΩ with respect to one of the weights ùúÉùëó is:\n",
        "\n",
        "‚àáùúÉùëóùêΩ(ùúÉ)=1ùëö‚àëùëñ=1ùëö(‚Ñé(ùëñ)‚àíùë¶(ùëñ))ùë•ùëó(5)\n",
        "'i' is the index across all 'm' training examples.\n",
        "\n",
        "'j' is the index of the weight ùúÉùëó, so ùë•ùëó is the feature associated with weight ùúÉùëó\n",
        "To update the weight ùúÉùëó, we adjust it by subtracting a fraction of the gradient determined by ùõº:\n",
        "\n",
        "ùúÉùëó=ùúÉùëó‚àíùõº√ó‚àáùúÉùëóùêΩ(ùúÉ)\n",
        "\n",
        "The learning rate ùõº is a value that we choose to control how big a single update will be.\n",
        "\n",
        "Instructions: Implement gradient descent function\n",
        "The number of iterations num_iters is the number of times that you'll use the entire training set.\n",
        "For each iteration, you'll calculate the cost function using all training examples (there are m training examples), and for all features.\n",
        "Instead of updating a single weight ùúÉùëñ at a time, we can update all the weights in the column vector:\n",
        "\n",
        "ùúÉ=ÓÄÇÓÄÄÓÄÅÓÄÅÓÄÅÓÄÅÓÄÅÓÄÅùúÉ0ùúÉ1ùúÉ2‚ãÆùúÉùëõÓÄÖÓÄÉÓÄÑÓÄÑÓÄÑÓÄÑÓÄÑÓÄÑ\n",
        "\n",
        "ùúÉ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term ùúÉ0 (note that the corresponding feature value ùê±0 is 1).\n",
        "\n",
        "The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'. ùëß=ùê±ùúÉ\n",
        "\n",
        "ùê± has dimensions (m, n+1)\n",
        "\n",
        "ùúÉ: has dimensions (n+1, 1)\n",
        "\n",
        "ùê≥: has dimensions (m, 1)\n",
        "\n",
        "The prediction 'h', is calculated by applying the sigmoid to each element in 'z': ‚Ñé(ùëß)=ùë†ùëñùëîùëöùëúùëñùëë(ùëß), and has dimensions (m,1).\n",
        "\n",
        "The cost function ùêΩ is calculated by taking the dot product of the vectors 'y' and 'log(h)'. Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
        "\n",
        "ùêΩ=‚àí1ùëö√ó(ùê≤ùëá‚ãÖùëôùëúùëî(ùê°)+(1‚àíùê≤)ùëá‚ãÖùëôùëúùëî(1‚àíùê°))\n",
        "\n",
        "The update of theta is also vectorized. Because the dimensions of ùê± are (m, n+1), and both ùê° and ùê≤ are (m, 1), we need to transpose the ùê± and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
        "\n",
        "ùúÉ=ùúÉ‚àíùõºùëö√ó(ùê±ùëá‚ãÖ(ùê°‚àíùê≤))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI5yssprLM7Y"
      },
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    This function should return (J, theta) by taking (x, y, theta, alpha, num_iters) as inputs, where\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    '''\n",
        "    \n",
        "    # get 'm', the number of rows in matrix x\n",
        "    m = x.shape[0]\n",
        "    \n",
        "    for i in range(0, num_iters):\n",
        "        \n",
        "        # get z, the dot product of x and theta\n",
        "        z = np.dot(x, theta)\n",
        "        \n",
        "        # get the sigmoid of z\n",
        "        h = sigmoid(z)\n",
        "       \n",
        "        \n",
        "        # calculate the cost function\n",
        "        J =-1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))  \n",
        "\n",
        "        # update the weights theta\n",
        "        theta = theta - (alpha/m)*(np.dot(np.transpose(x),(h-y)))\n",
        "        \n",
        "    ### END CODE HERE ###\n",
        "    J = float(J)\n",
        "    return J, theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1w_rhB_MYIo"
      },
      "source": [
        "# expected output should match with your output\n",
        "# Check the function\n",
        "# Construct a synthetic test case using numpy PRNG functions\n",
        "np.random.seed(1)\n",
        "# X input is 10 x 3 with ones for the bias terms\n",
        "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
        "# Y Labels are 10 x 1\n",
        "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
        "\n",
        "# Apply gradient descent\n",
        "# gradientDescent = np.vectorize(gradientDescent)\n",
        "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
        "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yoGuw_LMYTa"
      },
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    '''\n",
        "    This function should take tweet, freqs as input and return x, where:\n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "  \n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3)) \n",
        "    \n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1 \n",
        "    \n",
        "    \n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "        \n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,1] += freqs.get((word,1),0)\n",
        "        \n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,2] += freqs.get((word,0),0)\n",
        "        \n",
        "    assert(x.shape == (1, 3))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyO_DSaHYDnn"
      },
      "source": [
        "# Check your function\n",
        "# expected output should match with your output\n",
        "# test1: on training data\n",
        "tmp1 = extract_features(train_x[0], freqs)\n",
        "print(tmp1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9de4JrvLYD1L"
      },
      "source": [
        "# test 2:\n",
        "# expected output should match with your output\n",
        "# check for when the words are not in the freqs dictionary\n",
        "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
        "print(tmp2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50se5dbSYSCM"
      },
      "source": [
        "**Expected output**\n",
        "\n",
        "[[1. 0. 0.]]\n",
        "\n",
        "**Task 3: Training Your Model -** (5 Points)\n",
        "\n",
        "To train the model:\n",
        "\n",
        "Stack the features for all training examples into a matrix X.\n",
        "Call gradientDescent, which you've implemented above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E3cO6KqYD-b"
      },
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "# Expected output should map to your output\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500) #call gradient descent by appropriate parameters.\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jjhnGcCY0tf"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "The cost after training is 0.24216529.\n",
        "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n",
        "\n",
        "**Task 4**: Test your logistic regression - (10 Points)\n",
        "It is time for you to test your logistic regression function on some new input that your model has not seen before.\n",
        "\n",
        "**Instructions:** \n",
        "\n",
        "Write predict_tweet\n",
        "Predict whether a tweet is positive or negative.\n",
        "\n",
        "Given a tweet, process it, then extract the features.\n",
        "Apply the model's learned weights on the features to get the logits.\n",
        "Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
        "ùë¶ùëùùëüùëíùëë=ùë†ùëñùëîùëöùëúùëñùëë(ùê±‚ãÖùúÉ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn_L_M4WYEBT"
      },
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    This function should take (tweet, freqs, theta) as input and return y_pred as output where\n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    \n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet, freqs)    # call extract features function with appropriate parameters \n",
        "    \n",
        "    # make the prediction using x and theta by calling sigmoid function\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "    \n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKoRCVamYEHg"
      },
      "source": [
        "# Run this cell to test your function\n",
        "# Expected output should match to your output.\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmwdw0Tja3H7"
      },
      "source": [
        "Expected Output:\n",
        "\n",
        "I am happy -> 0.518580\n",
        "\n",
        "I am bad -> 0.494339\n",
        "\n",
        "this movie should have been great. -> 0.515331\n",
        "\n",
        "great -> 0.515464\n",
        "\n",
        "great great -> 0.530898\n",
        "\n",
        "great great great -> 0.546273\n",
        "\n",
        "great great great great -> 0.561561"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DAmQACVat42"
      },
      "source": [
        "my_tweet = \"@97sides CONGRATS :)\" # give your custom input and check \n",
        "predict_tweet(my_tweet, freqs, theta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrxWWftbJoz"
      },
      "source": [
        "Check performance using the test set\n",
        "After training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n",
        "\n",
        "**Instructions**: \n",
        "\n",
        "Implement test_logistic_regression\n",
        "Given the test data and the weights of your trained model, calculate the accuracy of your logistic regression model.\n",
        "Use your predict_tweet() function to make predictions on each tweet in the test set.\n",
        "\n",
        "If the prediction is > 0.5, set the model's classification y_hat to 1, otherwise set the model's classification y_hat to 0.\n",
        "A prediction is accurate when y_hat equals test_y. Sum up all the instances when they are equal and divide by m."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E79VGuIfauF-"
      },
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    This function should take (test_x, test_y, freqs, theta) as input and return accuracy as output, where\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    \n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)  # call predict tweet function with proper parameters\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1)\n",
        "            \n",
        "            \n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0)\n",
        "            \n",
        "\n",
        "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "    accuracy =np.sum((y_hat== np.squeeze(test_y)).astype(int))/len(test_x)     # use squeeze function of numpy array on test_y and use appropiate expression.\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzjCNvNiauKF"
      },
      "source": [
        "#Expected output should match to your output. \n",
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw9SJ32ObnSD"
      },
      "source": [
        "Expected Output:\n",
        "0.9950\n",
        "Pretty good!\n",
        "\n",
        "##Task 5: Predict with your own tweet - (5 points)¬∂"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suNhM5j6baoA"
      },
      "source": [
        "#Take a tweet and predict whether it is Positive Sentiment ot Negative Sentiment\n",
        "my_tweet =\"I need to stop looking at old soccer pictures :(\" #string containing the tweet\n",
        "print(process_tweet(my_tweet))\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)  # use appropriate function call to get prediction\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else: \n",
        "    print('Negative sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag47G-DfcDdo"
      },
      "source": [
        "##**PART-C**\n",
        "¬∂\n",
        "##**Theory:** (10 points)\n",
        "Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier? - 5 points\n",
        "\n",
        "How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the out‚Äê put layer, using what activation function? - 5 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ9eg_JoJ6lq"
      },
      "source": [
        "1)\n",
        " The logistic regression classifier is able to converge on non-linear data and outputs class probabilities. However, a classical perceptron will only converge if the data is linearly seperable.Also, it cannot compute class probabilities.\n",
        "\n",
        "2)\n",
        "\n",
        "Since it is a binary classification problem, you only need one neuron in the output layer. As for the activation function You'd have been using  the sigmoid in the output layer.\n",
        "\n",
        "For the MNIST problem you would need 10 output neurons in the final layer, one for each digit. The  activation function in this case would be softmax.\n",
        "\n"
      ]
    }
  ]
}