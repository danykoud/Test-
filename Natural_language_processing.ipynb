{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxJNyVgugyYUmyERqEaqgZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danykoud/Test-/blob/main/Natural_language_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p52AwMTZ1cOq"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "import re\n",
        "import string\n",
        "from os import getcwd\n",
        "\n",
        "import pdb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tkinter import *\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "import matplotlib.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxVP16HK8j0R"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1jyI9O21oiF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrVaoeW88qoO"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vc31dnTL4Xmi",
        "outputId": "8e6d3d25-cb85-4d88-fbf8-851a0c282dde"
      },
      "source": [
        "# TASK 1 CELL\n",
        "\n",
        "# get the sets of positive and negative tweets\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "# for i in positive_tweets:\n",
        "#   print(i)\n",
        "for j in positive_tweets:\n",
        "  print(j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtwitter_samples\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('twitter_samples')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/twitter_samples.zip/twitter_samples/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c43c2ff0c301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# get the sets of positive and negative tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpositive_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'positive_tweets.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnegative_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'negative_tweets.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# for i in positive_tweets:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtwitter_samples\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('twitter_samples')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/twitter_samples\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34M3iDvV5Gyw"
      },
      "source": [
        "label_positive = [1] * len(positive_tweets)\n",
        "label_negative = [0] * len(negative_tweets)\n",
        "print(label_positive)\n",
        "print(label_negative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjkGOZn1sV4"
      },
      "source": [
        "# TASK 1 CELL\n",
        "\n",
        "# get the sets of positive and negative tweets\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "label_positive = [1] * len(positive_tweets)\n",
        "label_negative = [0] * len(negative_tweets)\n",
        "\n",
        "labels = label_positive + label_negative\n",
        "tweets = positive_tweets + negative_tweets\n",
        "\n",
        "# split the data into train and validation set\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(tweets, labels, train_size=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2n766qP5-t5"
      },
      "source": [
        "training_1 = pd.DataFrame({'tweets':train_x, 'labels':train_y})\n",
        "training_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuWNQGlw6sBh"
      },
      "source": [
        "training = pd.DataFrame.from_dict(dict(tweet=train_x, label=train_y))\n",
        "training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms-K2nDC6Clk"
      },
      "source": [
        "sample = training[(training.tweet.str.contains(\"beautiful\")) &\n",
        "                      (training.tweet.str.contains(\"http\")) &\n",
        "                      (training.tweet.str.contains(\"#\"))]\n",
        "samp= sample.iloc[0].tweet\n",
        "samp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTk3ciFfqpu8"
      },
      "source": [
        "# TASK 1 CELL\n",
        "#remove hyperlinks, noise, punctuation, hashtags, old style retweet\n",
        "#text \"RT\", share market tickers like $FB, use stemming to only keep\n",
        "#track of one variation of each word and in the end will tokenize them. \n",
        "# result = re.sub(pattern, repl, string, count=0, flags=0);\n",
        "# result = re.sub('abc',  '',    input)           # Delete pattern abc\n",
        "# result = re.sub('abc',  'def', input)           # Replace pattern abc -> def\n",
        "# result = re.sub(r'\\s+', ' ',   input)           # Eliminate duplicate whitespaces using wildcards\n",
        "# result = re.sub('abc(def)ghi', r'\\1', input)    # Replace a string with a part of itself\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    '''\n",
        "    Params:\n",
        "        tweet: a string containing a tweet\n",
        "    Return:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "\n",
        "    # it will remove the old style retweet text \"RT\"\n",
        "    tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "    # it will remove hyperlinks\n",
        "    tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
        "\n",
        "    # it will remove hashtags. We have to be careful here not to remove \n",
        "    # the whole hashtag because text of hashtags contains huge information. \n",
        "    # only removing the hash # sign from the word\n",
        "    tweet2 = re.sub(r'#', '', tweet2)\n",
        "\n",
        "    # it will remove single numeric terms in the tweet. \n",
        "    tweet2 = re.sub(r'[0-9]', '', tweet2)\n",
        "    #print('\\nAfter removing old style tweet, hyperlinks and # sign')\n",
        "    #print(tweet2)\n",
        "    \n",
        "    #tweet2 = re.sub(r\"\\$\"+\"{}*\",format(.),'')\n",
        "    tweet2 = re.sub(r'\\$[\\w,]+\\b','', tweet2)\n",
        "\n",
        "    # instantiate the tokenizer class\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, \n",
        "                               strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "\n",
        "    # tokenize the tweets \n",
        "    tweet_tokens = tokenizer.tokenize(tweet2)\n",
        "\n",
        "    #print('\\nTokenized string:')\n",
        "    #print(tweet_tokens)\n",
        "    \n",
        "    #Import the english stop words list from NLTK\n",
        "    stopwords_english = stopwords.words('english') \n",
        "    \n",
        "    tweets_clean = []\n",
        "\n",
        "    for word in tweet_tokens: # Go through every word in your tokens list\n",
        "        if (word not in stopwords_english and \n",
        "            word not in string.punctuation):  \n",
        "            tweets_clean.append(word)\n",
        "\n",
        "    #print('\\n\\nAfter removing stop words and punctuation:')\n",
        "    #print(tweets_clean)\n",
        "    \n",
        "    # Instantiate stemming class\n",
        "    stemmer = PorterStemmer() \n",
        "\n",
        "    # Create an empty list to store the stems\n",
        "    tweets_stem = [] \n",
        "\n",
        "    for word in tweets_clean:\n",
        "        stem_word = stemmer.stem(word)  # stemming word\n",
        "        tweets_stem.append(stem_word)  # append to the list\n",
        "\n",
        "    #print('Words after stemming: ')\n",
        "    # print(tweets_stem)\n",
        "    \n",
        "    return tweets_stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AssTik1rrRAI"
      },
      "source": [
        "\n",
        "clean_tweet(samp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYK-IhLWr5qu"
      },
      "source": [
        "# TASK 1 CELL\n",
        "\n",
        "# Helper function to find word frequency\n",
        "def find(frequency, word, label):\n",
        "    '''\n",
        "    Params:\n",
        "        frequency: a dictionary with the frequency of each pair (or tuple)\n",
        "        word: the word to look up\n",
        "        label: the label corresponding to the word\n",
        "    Return:\n",
        "        n: the number of times the word with its corresponding label appears.\n",
        "    '''\n",
        "    n = 0  \n",
        "    \n",
        "    word_pair = (label, word)\n",
        "    print(word_pair)\n",
        "    #print(frequency.keys)\n",
        "    print(frequency)\n",
        "    #print(frequency[word_pair])\n",
        "    \n",
        "    \n",
        "    for word_pair in frequency.items():\n",
        "        \n",
        "        #n = frequency[word_pair]\n",
        "        n += 1\n",
        "        print('made it')\n",
        "        print(n)\n",
        "    \n",
        "    print(n)\n",
        "    return n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bw_zXZ8sjzb"
      },
      "source": [
        "# TASK 2 CELL\n",
        "\n",
        "def tweet_counter(output, tweets, tweet_senti):\n",
        "    '''\n",
        "    Params:\n",
        "        output: a dictionary that will be used to map each pair to its frequency\n",
        "        tweets: a list of tweets\n",
        "        tweet_senti: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
        "    Return:\n",
        "        output: a dictionary mapping each pair to its frequency\n",
        "    '''\n",
        " \n",
        "    for label, tweet in zip(tweet_senti, tweets):\n",
        "        for word in clean_tweet(tweet):\n",
        "            if (word, label) in output.keys():\n",
        "                output[(word, label)] = output[(word, label)] + 1\n",
        "                \n",
        "            else: \n",
        "                output[(word, label)] = 1\n",
        "   \n",
        "    return output\n",
        "\n",
        "frequency_dict = tweet_counter({}, tweets, labels)\n",
        "frequency_dict_pos = tweet_counter({}, tweets, label_positive)\n",
        "frequency_dict_neg = tweet_counter({}, tweets, label_negative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEz942R1I0oi"
      },
      "source": [
        "frequency_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPXt8N_rKdu0"
      },
      "source": [
        "\n",
        "freqs= frequency_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-2ENzMx2LTQ"
      },
      "source": [
        "# TASK 3 CELL\n",
        "\n",
        "def train_naive_bayes(frequency_dict, train_x, train_y):\n",
        "    '''\n",
        "    Params:\n",
        "        frequency_dict: dictionary from (word, label) to how often the word appears\n",
        "        train_x: a list of tweets\n",
        "        train_y: a list of labels correponding to the tweets (0,1)\n",
        "    Return:\n",
        "        logprior: the log prior. (equation 3 above)\n",
        "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
        "    '''\n",
        "    loglikelihood = {}\n",
        "    class_priors={}\n",
        "    logprior = 0\n",
        "\n",
        "\n",
        "    # calculate V, the number of unique words in the vocabulary\n",
        "    vocab = frequency_dict.items()\n",
        "    V = len(vocab)\n",
        "\n",
        "    # calculate num_pos and num_neg - the total number of positive and negative words for all documents\n",
        "    num_pos = num_neg = 0\n",
        "    for pair in frequency_dict.keys():\n",
        "        # if the label is positive (greater than zero)\n",
        "      for n in labels:\n",
        "        if n > 0:\n",
        "\n",
        "            # Increment the number of positive words by the count for this (word, label) pair\n",
        "            num_pos += 1\n",
        "\n",
        "        # else, the label is negative\n",
        "        else:\n",
        "\n",
        "            # increment the number of negative words by the count for this (word,label) pair\n",
        "            num_neg += 1\n",
        "\n",
        "    # Calculate num_doc, the number of documents\n",
        "    \n",
        "    num_doc =train_size= len(train_y)\n",
        "    # N_pos is number of positive documents\n",
        "    N_pos = np.sum(train_y)\n",
        "    # N_neg is the number of negative documents\n",
        "    N_neg = num_doc - N_pos\n",
        "    \n",
        "     # the log odds ratio\n",
        "    logprior = np.log(N_pos) - np.log(N_neg)\n",
        "    \n",
        "\n",
        "    # For each word in the vocabulary...\n",
        "    for word in vocab:\n",
        "        # get the positive and negative frequency of the word\n",
        "        freq_pos = len(frequency_dict_pos)\n",
        "        freq_neg = len(frequency_dict_neg)\n",
        "        # calculate the probability that each word is positive, and negative\n",
        "        p_w_pos =  (freq_pos + 1)/(num_pos+ V)\n",
        "        p_w_neg = (freq_neg + 1)/(num_neg + V)\n",
        "\n",
        "        # calculate the log likelihood of the word\n",
        "        # need to check the equation/------->loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
        "        loglikelihood[word] =  np.log(p_w_pos) - np.log(p_w_neg)\n",
        "\n",
        "\n",
        "    return logprior, loglikelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_prs055A2ePO"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrfugzm8q2BW"
      },
      "source": [
        "# TASK 4 CELL\n",
        "\n",
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "    '''\n",
        "    Params:\n",
        "        tweet: a string\n",
        "        logprior: a number\n",
        "        loglikelihood: a dictionary of words mapping to numbers\n",
        "    Return:\n",
        "        total_prob: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
        "\n",
        "    '''\n",
        "    # process the tweet to get a list of words\n",
        "    word_l = [words for words in tweet]\n",
        "\n",
        "    # initialize probability to zero\n",
        "    total_prob = 0\n",
        "\n",
        "    # add the logprior\n",
        "    total_prob =logprior + sum(loglikelihood[word] for word in word_l if word in loglikelihood)\n",
        "\n",
        "\n",
        "\n",
        "    return total_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1QtZwbOu2wa"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "custom_tweet = 'I love NLP'\n",
        "prob = naive_bayes_predict(custom_tweet, logprior, loglikelihood)\n",
        "print('The expected output is', prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la3kDsfBykZ_"
      },
      "source": [
        "# TASK 5 CELL\n",
        "\n",
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        test_x: A list of tweets\n",
        "        test_y: the corresponding labels for the list of tweets\n",
        "        logprior: the logprior\n",
        "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
        "    Return:\n",
        "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
        "    \"\"\"\n",
        "    accuracy = 0  \n",
        "\n",
        "    \n",
        "    y_hats = []\n",
        "    for tweet in test_x:\n",
        "        # if the prediction is > 0\n",
        "        if None:\n",
        "            # the predicted class is 1\n",
        "            y_hat_i = 1\n",
        "        else:\n",
        "            # otherwise the predicted class is 0\n",
        "            y_hat_i = 0\n",
        "\n",
        "        # append the predicted class to the list y_hats\n",
        "        y_hats\n",
        "\n",
        "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
        "    # TASK 5 CELL\n",
        "\n",
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        test_x: A list of tweets\n",
        "        test_y: the corresponding labels for the list of tweets\n",
        "        logprior: the logprior\n",
        "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
        "    Return:\n",
        "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
        "    \"\"\"\n",
        "    accuracy = 0  \n",
        "\n",
        "    \n",
        "    y_hats = []\n",
        "    for tweet in test_x:\n",
        "        # if the prediction is > 0\n",
        "        if None:\n",
        "            # the predicted class is 1\n",
        "            y_hat_i = None\n",
        "        else:\n",
        "            # otherwise the predicted class is 0\n",
        "            y_hat_i = None\n",
        "\n",
        "        # append the predicted class to the list y_hats\n",
        "        y_hats = None\n",
        "\n",
        "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
        "    # TASK 5 CELL\n",
        "\n",
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "        test_x: A list of tweets\n",
        "        test_y: the corresponding labels for the list of tweets\n",
        "        logprior: the logprior\n",
        "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
        "    Return:\n",
        "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
        "    \"\"\"\n",
        "    accuracy = 0  \n",
        "\n",
        "    \n",
        "    y_hats = []\n",
        "    for tweet in test_x:\n",
        "        # if the prediction is > 0\n",
        "        if naive_bayes_predict(tweet, logprior, loglikelihood) < 0:\n",
        "            # the predicted class is 1\n",
        "            y_hat_i = 1\n",
        "        else:\n",
        "            # otherwise the predicted class is 0\n",
        "            y_hat_i = 0\n",
        "\n",
        "        # append the predicted class to the list y_hats\n",
        "        y_hats.append(y_hat_i)\n",
        "\n",
        "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
        "    \n",
        "    error = np.mean(np.abs(np.array(y_hats) - np.array(test_y)))\n",
        "    \n",
        "    accuracy = 1 - error \n",
        "\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbMLYOIXypoV"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}